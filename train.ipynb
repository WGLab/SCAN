{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "the device is cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "device    = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('the device is %s' % device)\n",
    "\n",
    "from myLibs.myEval import myEval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "20210413-test\n"
     ]
    }
   ],
   "source": [
    "shuffle_seed      = 37\n",
    "pos_size          = 250\n",
    "neg_size          = 250\n",
    "train_size        = 1000\n",
    "BATCHSIZE        = 500\n",
    "\n",
    "num_epochs = 10\n",
    "prefix     = \"20210413-test\"\n",
    "print(prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "label_feature = pd.read_csv(\"myData/Train/labeled.csv\").values\n",
    "label_target  = pd.read_csv(\"myData/Train/true_label.csv\").values.reshape(-1)\n",
    "\n",
    "pos_feature = label_feature[label_target == 1]\n",
    "pos_label   = label_target[label_target == 1]\n",
    "np.random.seed(shuffle_seed)\n",
    "np.random.shuffle(pos_feature)\n",
    "np.random.seed(shuffle_seed)\n",
    "np.random.shuffle(pos_label)\n",
    "\n",
    "neg_feature = label_feature[label_target == 0]\n",
    "neg_label   = label_target[label_target == 0]\n",
    "np.random.seed(shuffle_seed)\n",
    "np.random.shuffle(neg_feature)\n",
    "np.random.seed(shuffle_seed)\n",
    "np.random.shuffle(neg_label)\n",
    "\n",
    "train_x = np.array(list(pos_feature[0:pos_size]) +list(neg_feature[0:neg_size]))\n",
    "train_y = np.array(list(pos_label[0:pos_size]) + list(neg_label[0:neg_size]))\n",
    "valid_x = np.array(list(pos_feature[pos_size:]) + list(neg_feature[neg_size:]))\n",
    "valid_y = np.array(list(pos_label[pos_size:]) + list(neg_label[neg_size:]))\n",
    "\n",
    "trainDataset = TensorDataset(torch.Tensor(train_x[:, np.newaxis, :]), torch.Tensor(train_y).long())\n",
    "labeledLoader = DataLoader(dataset=trainDataset, batch_size = BATCHSIZE, shuffle=True)\n",
    "\n",
    "validDataset = TensorDataset(torch.Tensor(valid_x[:, np.newaxis, :]), torch.Tensor(valid_y).long())\n",
    "validLoader  = DataLoader(dataset=validDataset, batch_size = BATCHSIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabel_data  = pd.read_csv(\"myData/Train/unlabel.csv\").values\n",
    "unlabeledDataset = TensorDataset(torch.Tensor(unlabel_data[:, np.newaxis, :]))\n",
    "unlabeledLoader  = DataLoader(dataset=unlabeledDataset, batch_size=BATCHSIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from myLibs.myDis import NetD\n",
    "from myLibs.myGen import NetG\n",
    "modelCD     = NetD().to(device)\n",
    "modelG      = NetG().to(device)\n",
    "criterionC  = nn.CrossEntropyLoss()\n",
    "optimizerCD = optim.Adam(modelCD.parameters(), lr=0.00095, betas=(0.5, 0.999), weight_decay = .01)\n",
    "optimizerG  = optim.Adam(modelG.parameters(),  lr=0.00095, betas=(0.5, 0.999), weight_decay = .01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "CNN1d_5layer(\n  (cnn1): Sequential(\n    (0): Conv1d(1, 10, kernel_size=(3,), stride=(1,))\n    (1): BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): Tanh()\n    (3): Dropout(p=0.6, inplace=False)\n  )\n  (cnn2): Sequential(\n    (0): Conv1d(10, 40, kernel_size=(3,), stride=(1,))\n    (1): BatchNorm1d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): Tanh()\n    (3): Dropout(p=0.6, inplace=False)\n  )\n  (cnn3): Sequential(\n    (0): Conv1d(40, 200, kernel_size=(3,), stride=(1,))\n    (1): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  )\n  (linear1): Sequential(\n    (0): Linear(in_features=13000, out_features=2, bias=True)\n  )\n)\nNetG(\n  (Linear1): Sequential(\n    (0): Linear(in_features=30, out_features=128, bias=True)\n    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): LeakyReLU(negative_slope=0.01)\n    (3): Dropout(p=0.6, inplace=False)\n  )\n  (linear2): Sequential(\n    (0): Linear(in_features=128, out_features=256, bias=True)\n    (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): LeakyReLU(negative_slope=0.01)\n    (3): Dropout(p=0.6, inplace=False)\n  )\n  (linear3): Sequential(\n    (0): Linear(in_features=256, out_features=512, bias=True)\n    (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): LeakyReLU(negative_slope=0.01)\n    (3): Dropout(p=0.6, inplace=False)\n  )\n  (linear4): Sequential(\n    (0): Linear(in_features=512, out_features=71, bias=True)\n    (1): BatchNorm1d(71, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): Tanh()\n  )\n)\n"
     ]
    }
   ],
   "source": [
    "print(modelCD)\n",
    "print(modelG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 10/10 [01:15<00:00,  7.55s/it]\n"
     ]
    }
   ],
   "source": [
    "from tensorboardX import SummaryWriter\n",
    "writer = SummaryWriter(comment=\"%s\" % prefix)\n",
    "\n",
    "maxacc = 0\n",
    "\n",
    "\n",
    "global_step = 1\n",
    "iter_labeled = iter(labeledLoader)\n",
    "\n",
    "\n",
    "for epoch in tqdm.tqdm(range(num_epochs)):\n",
    "    for step, x_unlabeled in enumerate(unlabeledLoader):\n",
    "\n",
    "        #################################################################################  Classifier/Discriminator \n",
    "        modelCD.train()\n",
    "        modelG.eval()\n",
    "        \n",
    "        optimizerCD.zero_grad()\n",
    "        \n",
    "        ## label\n",
    "        try:\n",
    "            x_labeled, y_labeled = next(iter_labeled)\n",
    "            x_labeled, y_labeled = x_labeled.to(device), y_labeled.to(device)\n",
    "        except StopIteration:\n",
    "            iter_labeled = iter(labeledLoader)\n",
    "            x_labeled, y_labeled = next(iter_labeled)\n",
    "            x_labeled, y_labeled = x_labeled.to(device), y_labeled.to(device)\n",
    "        \n",
    "        \n",
    "        _, outClassLabeled  = modelCD(x_labeled)\n",
    "        lossLabeled      = criterionC(outClassLabeled, y_labeled)\n",
    "        \n",
    "        \n",
    "        ## unlabel\n",
    "        x_unlabeled = x_unlabeled[0].to(device)\n",
    "        _, outClassUnlabeled  = modelCD(x_unlabeled)\n",
    "        \n",
    "        logz_unlabeled = torch.logsumexp(outClassUnlabeled, dim=1)\n",
    "        lossUnlabeled  = -0.5 * torch.mean(logz_unlabeled) + 0.5 * torch.mean(F.softplus(logz_unlabeled))\n",
    "        \n",
    "        ## Fake\n",
    "        fakeNoise1       = torch.randn(x_unlabeled.size(0), 30, device=device)\n",
    "        x_Fake1          = ( modelG(fakeNoise1) + 1.0 ) / 2\n",
    "        _, outClassFake1 = modelCD(x_Fake1)\n",
    "\n",
    "        logz_fake1 = torch.logsumexp(outClassFake1, dim=1)\n",
    "        lossFake  = 0.5 * torch.mean(F.softplus(logz_fake1))\n",
    "        \n",
    "\n",
    "        ## loss\n",
    "        totalLoss = lossLabeled + lossUnlabeled + lossFake\n",
    "        \n",
    "        ## optimization\n",
    "        writer.add_scalar(\"training_loss/supervised\", lossLabeled, global_step)\n",
    "        writer.add_scalar(\"training_loss/unsupervised\", lossUnlabeled+lossFake, global_step)\n",
    "        writer.add_scalar(\"training_loss/D_Loss\", totalLoss, global_step)\n",
    "\n",
    "        totalLoss.backward()\n",
    "        optimizerCD.step()\n",
    "        \n",
    "                \n",
    "        #################################################################################  Generator\n",
    "        modelCD.eval()\n",
    "        modelG.train()\n",
    "        optimizerG.zero_grad()\n",
    "        \n",
    "        fakeNoise2 = torch.randn(x_unlabeled.size(0), 30, device=device)\n",
    "        x_Fake2    = ( modelG(fakeNoise2) + 1.0 ) / 2\n",
    "        \n",
    "        ## loss\n",
    "        y_pred_unlabeled, _ = modelCD(x_unlabeled)\n",
    "        y_pred_fake, _      = modelCD(x_Fake2)\n",
    "        mom_real = torch.mean(y_pred_unlabeled, dim=0)\n",
    "        mom_fake = torch.mean(y_pred_fake, dim=0)\n",
    "        diff = mom_fake * 100 - mom_real * 100\n",
    "        lossG = torch.mean(diff * diff)\n",
    "        \n",
    "        \n",
    "        ## optimization\n",
    "        writer.add_scalar(\"training_loss/G_loss\", lossG, global_step)\n",
    "        lossG.backward()        \n",
    "        optimizerG.step()\n",
    "\n",
    "        global_step += 1\n",
    "  \n",
    "    \n",
    "    \n",
    "    train_loss, train_accuracy = myEval(modelCD, criterionC, device, labeledLoader, False)\n",
    "    writer.add_scalar(\"accuracy/train\", train_accuracy, epoch)\n",
    "\n",
    "    \n",
    "    if epoch >= 1000 and train_accuracy > maxacc:\n",
    "        torch.save(modelCD.state_dict(), './model_save/%s_%s.pt' % (prefix, epoch))\n",
    "        maxacc = train_accuracy\n",
    "\n",
    "\n",
    "    \n",
    "writer.flush()\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(modelCD.state_dict(), './model_save/%s.pt' % prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "source": [
    "## the performance of testing data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from models.model_20210222_CNN3 import model3_1\n",
    "# modelCD     = model3_1().to(device)\n",
    "# modelCD.load_state_dict(torch.load('./model_save/20210319-CNN3-net8_train_size4000_batch_size500_1368.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, roc_curve, auc\n",
    "from sklearn.metrics import f1_score \n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def myEval(model, device, test_loader, display = False):\n",
    "    model.eval()\n",
    "    \n",
    "    target_list = []\n",
    "    output_list = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for testdata in test_loader:\n",
    "            data, target = testdata\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            _, output = model(data)\n",
    "            softmax2_score = [ math.exp(i[1]) / ( math.exp(i[0]) + math.exp(i[1]) ) for i in output.cpu().numpy() ]\n",
    "            target_list += target.cpu().tolist()\n",
    "            output_list += softmax2_score\n",
    "\n",
    "    return target_list, output_list\n",
    "\n",
    "\n",
    "def evaluation_df(pred_score, labeled_y):\n",
    "    def TP_table(pred_score, labeled_y, threshold):\n",
    "        y_pred = [0 if i < threshold else 1 for i in pred_score]\n",
    "        y_true = list(labeled_y)\n",
    "\n",
    "\n",
    "        fpr, tpr, _ = roc_curve(y_true, pred_score)\n",
    "        auc_val = auc(fpr, tpr)\n",
    "\n",
    "\n",
    "        tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "        # TP TN FP FN sensitivity specificity Accuracy\n",
    "        sensitivity = tp/(tp+fn)\n",
    "\n",
    "        specificity = tn/(tn+fp)\n",
    "        accuracy    = (tp+tn)/(tp+tn+fp+fn)\n",
    "        \n",
    "        F1 = f1_score(y_true, y_pred)\n",
    "        \n",
    "        try:\n",
    "            MCC = ((tp*tn)-(fp*fn)) / ((tp+fp)*(tp+fn)*(tn+fp)*(tn+fn))**0.5\n",
    "        except:\n",
    "            MCC = np.nan\n",
    "\n",
    "        return [threshold, tp, fp, tn, fn, sensitivity, specificity, accuracy, auc_val, MCC, F1]\n",
    "\n",
    "    res = []\n",
    "    for i in range(1,20):\n",
    "        threshold = i / 20\n",
    "        res.append(TP_table(pred_score, labeled_y, threshold))\n",
    "\n",
    "    res = pd.DataFrame(res, columns=['threshold', 'TP', 'FP', 'TN', 'FN', 'sen', 'spe', 'Acc', 'AUC', 'MCC', 'F1'])\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "    threshold    TP    FP    TN   FN       sen       spe       Acc       AUC  \\\n",
       "0        0.05  1257  3222  1669   78  0.941573  0.341239  0.469965  0.789877   \n",
       "1        0.10  1243  3076  1815   92  0.931086  0.371090  0.491166  0.789877   \n",
       "2        0.15  1236  3003  1888   99  0.925843  0.386015  0.501767  0.789877   \n",
       "3        0.20  1235  2952  1939  100  0.925094  0.396442  0.509798  0.789877   \n",
       "4        0.25  1228  2906  1985  107  0.919850  0.405847  0.516062  0.789877   \n",
       "5        0.30  1223  2869  2022  112  0.916105  0.413412  0.521201  0.789877   \n",
       "6        0.35  1218  2809  2082  117  0.912360  0.425680  0.530035  0.789877   \n",
       "7        0.40  1216  2773  2118  119  0.910861  0.433040  0.535496  0.789877   \n",
       "8        0.45  1212  2741  2150  123  0.907865  0.439583  0.539994  0.789877   \n",
       "9        0.50  1210  2706  2185  125  0.906367  0.446739  0.545294  0.789877   \n",
       "10       0.55  1205  2671  2220  130  0.902622  0.453895  0.550112  0.789877   \n",
       "11       0.60  1199  2629  2262  136  0.898127  0.462482  0.555895  0.789877   \n",
       "12       0.65  1193  2602  2289  142  0.893633  0.468002  0.559268  0.789877   \n",
       "13       0.70  1188  2573  2318  147  0.889888  0.473932  0.563122  0.789877   \n",
       "14       0.75  1175  2518  2373  160  0.880150  0.485177  0.569868  0.789877   \n",
       "15       0.80  1172  2473  2418  163  0.877903  0.494377  0.576614  0.789877   \n",
       "16       0.85  1162  2396  2495  173  0.870412  0.510121  0.587376  0.789877   \n",
       "17       0.90  1140  2259  2632  195  0.853933  0.538131  0.605846  0.789877   \n",
       "18       0.95  1095  1995  2896  240  0.820225  0.592108  0.641022  0.789877   \n",
       "\n",
       "         MCC        F1  \n",
       "0   0.258345  0.432405  \n",
       "1   0.269050  0.439689  \n",
       "2   0.274578  0.443488  \n",
       "3   0.281196  0.447302  \n",
       "4   0.283001  0.449077  \n",
       "5   0.284939  0.450709  \n",
       "6   0.290271  0.454308  \n",
       "7   0.294177  0.456799  \n",
       "8   0.296187  0.458396  \n",
       "9   0.299997  0.460865  \n",
       "10  0.301852  0.462483  \n",
       "11  0.304135  0.464459  \n",
       "12  0.304237  0.465107  \n",
       "13  0.305327  0.466248  \n",
       "14  0.305220  0.467383  \n",
       "15  0.310146  0.470683  \n",
       "16  0.315599  0.474964  \n",
       "17  0.323190  0.481622  \n",
       "18  0.338470  0.494915  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>threshold</th>\n      <th>TP</th>\n      <th>FP</th>\n      <th>TN</th>\n      <th>FN</th>\n      <th>sen</th>\n      <th>spe</th>\n      <th>Acc</th>\n      <th>AUC</th>\n      <th>MCC</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.05</td>\n      <td>1257</td>\n      <td>3222</td>\n      <td>1669</td>\n      <td>78</td>\n      <td>0.941573</td>\n      <td>0.341239</td>\n      <td>0.469965</td>\n      <td>0.789877</td>\n      <td>0.258345</td>\n      <td>0.432405</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.10</td>\n      <td>1243</td>\n      <td>3076</td>\n      <td>1815</td>\n      <td>92</td>\n      <td>0.931086</td>\n      <td>0.371090</td>\n      <td>0.491166</td>\n      <td>0.789877</td>\n      <td>0.269050</td>\n      <td>0.439689</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.15</td>\n      <td>1236</td>\n      <td>3003</td>\n      <td>1888</td>\n      <td>99</td>\n      <td>0.925843</td>\n      <td>0.386015</td>\n      <td>0.501767</td>\n      <td>0.789877</td>\n      <td>0.274578</td>\n      <td>0.443488</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.20</td>\n      <td>1235</td>\n      <td>2952</td>\n      <td>1939</td>\n      <td>100</td>\n      <td>0.925094</td>\n      <td>0.396442</td>\n      <td>0.509798</td>\n      <td>0.789877</td>\n      <td>0.281196</td>\n      <td>0.447302</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.25</td>\n      <td>1228</td>\n      <td>2906</td>\n      <td>1985</td>\n      <td>107</td>\n      <td>0.919850</td>\n      <td>0.405847</td>\n      <td>0.516062</td>\n      <td>0.789877</td>\n      <td>0.283001</td>\n      <td>0.449077</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0.30</td>\n      <td>1223</td>\n      <td>2869</td>\n      <td>2022</td>\n      <td>112</td>\n      <td>0.916105</td>\n      <td>0.413412</td>\n      <td>0.521201</td>\n      <td>0.789877</td>\n      <td>0.284939</td>\n      <td>0.450709</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0.35</td>\n      <td>1218</td>\n      <td>2809</td>\n      <td>2082</td>\n      <td>117</td>\n      <td>0.912360</td>\n      <td>0.425680</td>\n      <td>0.530035</td>\n      <td>0.789877</td>\n      <td>0.290271</td>\n      <td>0.454308</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0.40</td>\n      <td>1216</td>\n      <td>2773</td>\n      <td>2118</td>\n      <td>119</td>\n      <td>0.910861</td>\n      <td>0.433040</td>\n      <td>0.535496</td>\n      <td>0.789877</td>\n      <td>0.294177</td>\n      <td>0.456799</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>0.45</td>\n      <td>1212</td>\n      <td>2741</td>\n      <td>2150</td>\n      <td>123</td>\n      <td>0.907865</td>\n      <td>0.439583</td>\n      <td>0.539994</td>\n      <td>0.789877</td>\n      <td>0.296187</td>\n      <td>0.458396</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>0.50</td>\n      <td>1210</td>\n      <td>2706</td>\n      <td>2185</td>\n      <td>125</td>\n      <td>0.906367</td>\n      <td>0.446739</td>\n      <td>0.545294</td>\n      <td>0.789877</td>\n      <td>0.299997</td>\n      <td>0.460865</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>0.55</td>\n      <td>1205</td>\n      <td>2671</td>\n      <td>2220</td>\n      <td>130</td>\n      <td>0.902622</td>\n      <td>0.453895</td>\n      <td>0.550112</td>\n      <td>0.789877</td>\n      <td>0.301852</td>\n      <td>0.462483</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>0.60</td>\n      <td>1199</td>\n      <td>2629</td>\n      <td>2262</td>\n      <td>136</td>\n      <td>0.898127</td>\n      <td>0.462482</td>\n      <td>0.555895</td>\n      <td>0.789877</td>\n      <td>0.304135</td>\n      <td>0.464459</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>0.65</td>\n      <td>1193</td>\n      <td>2602</td>\n      <td>2289</td>\n      <td>142</td>\n      <td>0.893633</td>\n      <td>0.468002</td>\n      <td>0.559268</td>\n      <td>0.789877</td>\n      <td>0.304237</td>\n      <td>0.465107</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>0.70</td>\n      <td>1188</td>\n      <td>2573</td>\n      <td>2318</td>\n      <td>147</td>\n      <td>0.889888</td>\n      <td>0.473932</td>\n      <td>0.563122</td>\n      <td>0.789877</td>\n      <td>0.305327</td>\n      <td>0.466248</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>0.75</td>\n      <td>1175</td>\n      <td>2518</td>\n      <td>2373</td>\n      <td>160</td>\n      <td>0.880150</td>\n      <td>0.485177</td>\n      <td>0.569868</td>\n      <td>0.789877</td>\n      <td>0.305220</td>\n      <td>0.467383</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>0.80</td>\n      <td>1172</td>\n      <td>2473</td>\n      <td>2418</td>\n      <td>163</td>\n      <td>0.877903</td>\n      <td>0.494377</td>\n      <td>0.576614</td>\n      <td>0.789877</td>\n      <td>0.310146</td>\n      <td>0.470683</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>0.85</td>\n      <td>1162</td>\n      <td>2396</td>\n      <td>2495</td>\n      <td>173</td>\n      <td>0.870412</td>\n      <td>0.510121</td>\n      <td>0.587376</td>\n      <td>0.789877</td>\n      <td>0.315599</td>\n      <td>0.474964</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>0.90</td>\n      <td>1140</td>\n      <td>2259</td>\n      <td>2632</td>\n      <td>195</td>\n      <td>0.853933</td>\n      <td>0.538131</td>\n      <td>0.605846</td>\n      <td>0.789877</td>\n      <td>0.323190</td>\n      <td>0.481622</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>0.95</td>\n      <td>1095</td>\n      <td>1995</td>\n      <td>2896</td>\n      <td>240</td>\n      <td>0.820225</td>\n      <td>0.592108</td>\n      <td>0.641022</td>\n      <td>0.789877</td>\n      <td>0.338470</td>\n      <td>0.494915</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "label_feature = pd.read_csv(\"myData/Test/labeled.csv\").values\n",
    "label_target  = pd.read_csv(\"myData/Test/true_label.csv\").values.reshape(-1)\n",
    "\n",
    "\n",
    "testDataset = TensorDataset(torch.Tensor(label_feature[:, np.newaxis, :]), torch.Tensor(label_target).long())\n",
    "testLoader = DataLoader(dataset=testDataset, batch_size = BATCHSIZE, shuffle=True)\n",
    "\n",
    "label, pred = myEval(modelCD, device, testLoader)\n",
    "evaluation_df(pred, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python385jvsc74a57bd09201720806e540b32a3ce99c29deb1f8a318d0ec5df15c246eb4449a19fd2da9",
   "display_name": "Python 3.8.5 64-bit ('pytorch': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}