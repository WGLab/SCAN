{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clinical interpretation of cancer somatic mutatios using Semi-Supervised Generative Adversarial Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the device is cuda\n"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "device    = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('the device is %s' % device)\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "prefix = 'a-test'\n",
    "\n",
    "\n",
    "from myLibs import myTrainPrep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load the training data\n",
    "\n",
    "Note:\n",
    "* When the labeled batch size is 4k and unlabeled batch size is 10k, the used memory of GPU is ~6Gb.\n",
    "* Training 2000 epochs with 200 batch size on NVIDIA 1080Ti, the whole process will take 4 hours. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffle_seed      = 37\n",
    "\n",
    "## the size of labeled data for training\n",
    "pos_size          = 1000\n",
    "neg_size          = 3000\n",
    "train_size        = pos_size + neg_size\n",
    "\n",
    "## batch size\n",
    "batch_size        = 4000\n",
    "\n",
    "## no. of epoch\n",
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1668, 71)\n",
      "(3566, 71)\n",
      "(4000, 71)\n"
     ]
    }
   ],
   "source": [
    "label_feature = pd.read_csv(\"./myData/Marilyn-std0.02/labeled.csv\").values\n",
    "label_target  = pd.read_csv(\"./myData/Marilyn-std0.02/true_label.csv\").values.reshape(-1)\n",
    "\n",
    "pos_feature = label_feature[label_target == 1]\n",
    "pos_label   = label_target[label_target == 1]\n",
    "print(pos_feature.shape)\n",
    "neg_feature = label_feature[label_target == 0]\n",
    "neg_label   = label_target[label_target == 0]\n",
    "print(neg_feature.shape)\n",
    "\n",
    "train_x = np.array(list(pos_feature[0:pos_size]) + list(neg_feature[0:neg_size]))\n",
    "print(train_x.shape)\n",
    "train_y = np.array(list(pos_label[0:pos_size])   + list(neg_label[0:neg_size]))\n",
    "\n",
    "np.random.seed(shuffle_seed)\n",
    "np.random.shuffle(train_x)\n",
    "np.random.seed(shuffle_seed)\n",
    "np.random.shuffle(train_y)\n",
    "\n",
    "\n",
    "valid_x = np.array(list(pos_feature[pos_size:]) + list(neg_feature[neg_size:]))\n",
    "valid_y = np.array(list(pos_label[pos_size:]) + list(neg_label[neg_size:]))\n",
    "\n",
    "np.random.seed(shuffle_seed)\n",
    "np.random.shuffle(valid_x)\n",
    "np.random.seed(shuffle_seed)\n",
    "np.random.shuffle(valid_y)\n",
    "\n",
    "labeled_loader = myTrainPrep.labeled_batch_creator(train_x, train_y, batch_size, -1)\n",
    "valided_loader = myTrainPrep.labeled_batch_creator(valid_x, valid_y, batch_size, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load unlabeled data\n",
    "Here, we used 60k unlabeled data in training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabel_data  = pd.read_csv(\"./myData/Marilyn-std0.02/unlabel.csv\").sample(n=60000, replace=False, axis=0, random_state=1).values\n",
    "\n",
    "np.random.seed(shuffle_seed)\n",
    "np.random.shuffle(unlabel_data)\n",
    "unlabeled_loader = myTrainPrep.unlabeled_batch_creator(unlabel_data, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN1d_5layer(\n",
      "  (cnn1): Sequential(\n",
      "    (0): Conv1d(1, 10, kernel_size=(3,), stride=(1,))\n",
      "    (1): BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): Tanh()\n",
      "    (3): Dropout(p=0.3, inplace=False)\n",
      "  )\n",
      "  (cnn2): Sequential(\n",
      "    (0): Conv1d(10, 40, kernel_size=(3,), stride=(1,))\n",
      "    (1): BatchNorm1d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): Tanh()\n",
      "    (3): Dropout(p=0.3, inplace=False)\n",
      "  )\n",
      "  (cnn3): Sequential(\n",
      "    (0): Conv1d(40, 200, kernel_size=(3,), stride=(1,))\n",
      "    (1): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (linear1): Sequential(\n",
      "    (0): Linear(in_features=13000, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "NetG7(\n",
      "  (ln1): LayerNorm((128,), eps=1e-05, elementwise_affine=False)\n",
      "  (ln2): LayerNorm((256,), eps=1e-05, elementwise_affine=False)\n",
      "  (ln3): LayerNorm((512,), eps=1e-05, elementwise_affine=False)\n",
      "  (Linear1): Sequential(\n",
      "    (0): Linear(in_features=30, out_features=128, bias=True)\n",
      "    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): LeakyReLU(negative_slope=0.01)\n",
      "    (3): Dropout(p=0.6, inplace=False)\n",
      "  )\n",
      "  (linear2): Sequential(\n",
      "    (0): Linear(in_features=128, out_features=256, bias=True)\n",
      "    (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): LeakyReLU(negative_slope=0.01)\n",
      "    (3): Dropout(p=0.6, inplace=False)\n",
      "  )\n",
      "  (linear3): Sequential(\n",
      "    (0): Linear(in_features=256, out_features=512, bias=True)\n",
      "    (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): LeakyReLU(negative_slope=0.01)\n",
      "    (3): Dropout(p=0.6, inplace=False)\n",
      "  )\n",
      "  (linear4): Sequential(\n",
      "    (0): Linear(in_features=512, out_features=71, bias=True)\n",
      "    (1): BatchNorm1d(71, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): Tanh()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)\n",
    "\n",
    "\n",
    "from myLibs.myGen import NetG7\n",
    "from myLibs.myDis import model3_1\n",
    "modelCD     = model3_1().to(device)\n",
    "modelG      = NetG7(batch_size).to(device)\n",
    "modelCD.apply(weights_init)\n",
    "modelG.apply(weights_init)\n",
    "optimizerCD = optim.Adam(modelCD.parameters(), lr=0.00095, betas=(0.5, 0.999), weight_decay = .001)\n",
    "optimizerG  = optim.Adam(modelG.parameters(),  lr=0.00095, betas=(0.5, 0.999), weight_decay = .001)\n",
    "\n",
    "print(modelCD)\n",
    "print(modelG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training process\n",
    "\n",
    "- We use [tensorboardX](https://pytorch.org/docs/stable/tensorboard.html) to track and visulize the losses and accuracies during training.\n",
    "- The loss functions used in this training process are based on the paper [Improved Techniques for Training GANs](https://arxiv.org/abs/1606.03498)\n",
    "\n",
    "\n",
    "To visulize the losses using tensorboardx:\n",
    "```bash\n",
    "tensorboard --logdir=runs\n",
    "```\n",
    "And then click the link."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [01:11<00:00,  7.13s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "writer = SummaryWriter(comment=\"%s\" % prefix)\n",
    "\n",
    "global_step = 1\n",
    "\n",
    "for epoch in tqdm.tqdm(range(num_epochs)):\n",
    "    for step in range(len(unlabeled_loader)):\n",
    "        #################################################################################  Discriminator\n",
    "        modelCD.train()\n",
    "        modelG.eval()\n",
    "        optimizerCD.zero_grad()\n",
    "\n",
    "        ## label\n",
    "        labeled_dict     = labeled_loader[0]\n",
    "        x_labeled        = labeled_dict['data'][:, np.newaxis, :].to(device)\n",
    "        y_labeled        = labeled_dict['target'].to(device)\n",
    "        y_pred_labeled   = modelCD(x_labeled)  \n",
    "        pdist   = y_pred_labeled.gather(1, y_labeled.view(-1, 1)).squeeze(1)\n",
    "        labeled = torch.logsumexp(y_pred_labeled, dim=1)\n",
    "        lossS   = -torch.mean(pdist) + torch.mean(labeled)\n",
    "        \n",
    "        \n",
    "        ## unlabel\n",
    "        unlabeled_dict  = unlabeled_loader[step]\n",
    "        x_unlabeled     = unlabeled_dict['data'][:, np.newaxis, :].to(device)\n",
    "        y_unlabeled     = unlabeled_dict['target'].to(device)\n",
    "        outClassUnlabeled  = modelCD(x_unlabeled)\n",
    "        \n",
    "        logz_unlabeled = torch.logsumexp(outClassUnlabeled, dim=1)\n",
    "        lossUnlabeled  = 0.5 * ( -torch.mean(logz_unlabeled) + torch.mean(F.softplus(logz_unlabeled)) )\n",
    "        \n",
    "        ## Fake\n",
    "        fakeNoise1 = torch.randn(x_unlabeled.size(0), 30, device=device)\n",
    "        x_Fake1    = ( modelG(fakeNoise1) + 1.0 ) / 2\n",
    "        outClassFake1 = modelCD(x_Fake1.detach())\n",
    "\n",
    "        logz_fake1 = torch.logsumexp(outClassFake1, dim=1)\n",
    "        lossFake  = 0.5 * torch.mean(F.softplus(logz_fake1))\n",
    "        \n",
    "        ## loss\n",
    "        lossU      = lossUnlabeled + lossFake\n",
    "        lossD = lossU + lossS\n",
    "        \n",
    "        writer.add_scalar(\"training_loss/supervised\", lossS, global_step)\n",
    "        writer.add_scalar(\"training_loss/unsupervised\", lossU, global_step)        \n",
    "        writer.add_scalar(\"training_loss/lossD\", lossD, global_step)\n",
    "        \n",
    "        \n",
    "        lossD.backward()\n",
    "        optimizerCD.step()\n",
    "        \n",
    "                \n",
    "        #################################################################################  Generator\n",
    "        modelCD.eval()\n",
    "        modelG.train()\n",
    "        optimizerG.zero_grad()\n",
    "        \n",
    "        ## loss\n",
    "        y_pred_unlabeled = modelCD(x_unlabeled)\n",
    "        y_pred_fake      = modelCD(x_Fake1)\n",
    "        \n",
    "        mom_real = torch.mean(y_pred_unlabeled, dim=0)\n",
    "        mom_fake = torch.mean(y_pred_fake, dim=0)\n",
    "        diff = mom_fake - mom_real\n",
    "        lossG = torch.mean(diff * diff)\n",
    "\n",
    "        \n",
    "        ## optimization\n",
    "        writer.add_scalar(\"training_loss/lossG\", lossG, global_step)\n",
    "        \n",
    "        \n",
    "        lossG.backward()        \n",
    "        optimizerG.step()\n",
    "        \n",
    "        \n",
    "        global_step += 1\n",
    "        \n",
    "    \n",
    "    \n",
    "    train_loss, train_accuracy = myTrainPrep.myPerform(modelCD, device, labeled_loader)\n",
    "    writer.add_scalar(\"accuracy/train\", train_accuracy, epoch)\n",
    "\n",
    "    test_loss, test_accuracy = myTrainPrep.myPerform(modelCD, device, valided_loader)\n",
    "    writer.add_scalar(\"accuracy/test\", test_accuracy, epoch)\n",
    "\n",
    "    \n",
    "writer.flush()\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(modelCD.state_dict(), './model_save/%s.pt' % title_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### validation performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc, f1_score\n",
    "\n",
    "\n",
    "def myEval(model, device, test_loader, display = False):\n",
    "    model.eval()\n",
    "    \n",
    "    target_list = []\n",
    "    output_list = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            data = batch['data'][:, np.newaxis, :].to(device)\n",
    "            target = batch['target'].to(device)\n",
    "            output = model(data)\n",
    "            softmax2_score = [ math.exp(i[1]) / ( math.exp(i[0]) + math.exp(i[1]) ) for i in output.cpu().numpy() ]\n",
    "            target_list += target.cpu().tolist()\n",
    "            output_list += softmax2_score\n",
    "\n",
    "    return target_list, output_list\n",
    "\n",
    "\n",
    "def evaluation_df(pred_score, labeled_y):\n",
    "    def TP_table(pred_score, labeled_y, threshold):\n",
    "        y_pred = [0 if i < threshold else 1 for i in pred_score]\n",
    "        y_true = list(labeled_y)\n",
    "\n",
    "\n",
    "        fpr, tpr, _ = roc_curve(y_true, pred_score)\n",
    "        auc_val = auc(fpr, tpr)\n",
    "\n",
    "\n",
    "        tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "        # TP TN FP FN sensitivity specificity Accuracy\n",
    "        sensitivity = tp/(tp+fn)\n",
    "\n",
    "        specificity = tn/(tn+fp)\n",
    "        accuracy    = (tp+tn)/(tp+tn+fp+fn)\n",
    "        \n",
    "        F1 = f1_score(y_true, y_pred)\n",
    "        \n",
    "        try:\n",
    "            MCC = ((tp*tn)-(fp*fn)) / ((tp+fp)*(tp+fn)*(tn+fp)*(tn+fn))**0.5\n",
    "        except:\n",
    "            MCC = np.nan\n",
    "\n",
    "        return [threshold, tp, fp, tn, fn, sensitivity, specificity, accuracy, auc_val, MCC, F1]\n",
    "\n",
    "    res = []\n",
    "    for i in range(1,20):\n",
    "        threshold = i / 20\n",
    "        res.append(TP_table(pred_score, labeled_y, threshold))\n",
    "\n",
    "    res = pd.DataFrame(res, columns=['threshold', 'TP', 'FP', 'TN', 'FN', 'sen', 'spe', 'Acc', 'AUC', 'MCC', 'F1'])\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>threshold</th>\n",
       "      <th>TP</th>\n",
       "      <th>FP</th>\n",
       "      <th>TN</th>\n",
       "      <th>FN</th>\n",
       "      <th>sen</th>\n",
       "      <th>spe</th>\n",
       "      <th>Acc</th>\n",
       "      <th>AUC</th>\n",
       "      <th>MCC</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.05</td>\n",
       "      <td>609</td>\n",
       "      <td>44</td>\n",
       "      <td>522</td>\n",
       "      <td>59</td>\n",
       "      <td>0.911677</td>\n",
       "      <td>0.922261</td>\n",
       "      <td>0.916532</td>\n",
       "      <td>0.969459</td>\n",
       "      <td>0.832503</td>\n",
       "      <td>0.922029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.10</td>\n",
       "      <td>577</td>\n",
       "      <td>25</td>\n",
       "      <td>541</td>\n",
       "      <td>91</td>\n",
       "      <td>0.863772</td>\n",
       "      <td>0.955830</td>\n",
       "      <td>0.905997</td>\n",
       "      <td>0.969459</td>\n",
       "      <td>0.817040</td>\n",
       "      <td>0.908661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.15</td>\n",
       "      <td>558</td>\n",
       "      <td>17</td>\n",
       "      <td>549</td>\n",
       "      <td>110</td>\n",
       "      <td>0.835329</td>\n",
       "      <td>0.969965</td>\n",
       "      <td>0.897083</td>\n",
       "      <td>0.969459</td>\n",
       "      <td>0.804404</td>\n",
       "      <td>0.897828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.20</td>\n",
       "      <td>545</td>\n",
       "      <td>15</td>\n",
       "      <td>551</td>\n",
       "      <td>123</td>\n",
       "      <td>0.815868</td>\n",
       "      <td>0.973498</td>\n",
       "      <td>0.888169</td>\n",
       "      <td>0.969459</td>\n",
       "      <td>0.790044</td>\n",
       "      <td>0.887622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.25</td>\n",
       "      <td>534</td>\n",
       "      <td>13</td>\n",
       "      <td>553</td>\n",
       "      <td>134</td>\n",
       "      <td>0.799401</td>\n",
       "      <td>0.977032</td>\n",
       "      <td>0.880875</td>\n",
       "      <td>0.969459</td>\n",
       "      <td>0.778804</td>\n",
       "      <td>0.879012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.30</td>\n",
       "      <td>521</td>\n",
       "      <td>12</td>\n",
       "      <td>554</td>\n",
       "      <td>147</td>\n",
       "      <td>0.779940</td>\n",
       "      <td>0.978799</td>\n",
       "      <td>0.871151</td>\n",
       "      <td>0.969459</td>\n",
       "      <td>0.763249</td>\n",
       "      <td>0.867610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.35</td>\n",
       "      <td>508</td>\n",
       "      <td>9</td>\n",
       "      <td>557</td>\n",
       "      <td>160</td>\n",
       "      <td>0.760479</td>\n",
       "      <td>0.984099</td>\n",
       "      <td>0.863047</td>\n",
       "      <td>0.969459</td>\n",
       "      <td>0.751972</td>\n",
       "      <td>0.857384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.40</td>\n",
       "      <td>498</td>\n",
       "      <td>9</td>\n",
       "      <td>557</td>\n",
       "      <td>170</td>\n",
       "      <td>0.745509</td>\n",
       "      <td>0.984099</td>\n",
       "      <td>0.854943</td>\n",
       "      <td>0.969459</td>\n",
       "      <td>0.738950</td>\n",
       "      <td>0.847660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.45</td>\n",
       "      <td>483</td>\n",
       "      <td>7</td>\n",
       "      <td>559</td>\n",
       "      <td>185</td>\n",
       "      <td>0.723054</td>\n",
       "      <td>0.987633</td>\n",
       "      <td>0.844408</td>\n",
       "      <td>0.969459</td>\n",
       "      <td>0.723752</td>\n",
       "      <td>0.834197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.50</td>\n",
       "      <td>465</td>\n",
       "      <td>6</td>\n",
       "      <td>560</td>\n",
       "      <td>203</td>\n",
       "      <td>0.696108</td>\n",
       "      <td>0.989399</td>\n",
       "      <td>0.830632</td>\n",
       "      <td>0.969459</td>\n",
       "      <td>0.703130</td>\n",
       "      <td>0.816506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.55</td>\n",
       "      <td>450</td>\n",
       "      <td>6</td>\n",
       "      <td>560</td>\n",
       "      <td>218</td>\n",
       "      <td>0.673653</td>\n",
       "      <td>0.989399</td>\n",
       "      <td>0.818476</td>\n",
       "      <td>0.969459</td>\n",
       "      <td>0.684497</td>\n",
       "      <td>0.800712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.60</td>\n",
       "      <td>439</td>\n",
       "      <td>5</td>\n",
       "      <td>561</td>\n",
       "      <td>229</td>\n",
       "      <td>0.657186</td>\n",
       "      <td>0.991166</td>\n",
       "      <td>0.810373</td>\n",
       "      <td>0.969459</td>\n",
       "      <td>0.673135</td>\n",
       "      <td>0.789568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.65</td>\n",
       "      <td>433</td>\n",
       "      <td>4</td>\n",
       "      <td>562</td>\n",
       "      <td>235</td>\n",
       "      <td>0.648204</td>\n",
       "      <td>0.992933</td>\n",
       "      <td>0.806321</td>\n",
       "      <td>0.969459</td>\n",
       "      <td>0.668001</td>\n",
       "      <td>0.783710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.70</td>\n",
       "      <td>419</td>\n",
       "      <td>3</td>\n",
       "      <td>563</td>\n",
       "      <td>249</td>\n",
       "      <td>0.627246</td>\n",
       "      <td>0.994700</td>\n",
       "      <td>0.795786</td>\n",
       "      <td>0.969459</td>\n",
       "      <td>0.653302</td>\n",
       "      <td>0.768807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.75</td>\n",
       "      <td>401</td>\n",
       "      <td>2</td>\n",
       "      <td>564</td>\n",
       "      <td>267</td>\n",
       "      <td>0.600299</td>\n",
       "      <td>0.996466</td>\n",
       "      <td>0.782010</td>\n",
       "      <td>0.969459</td>\n",
       "      <td>0.634085</td>\n",
       "      <td>0.748833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.80</td>\n",
       "      <td>386</td>\n",
       "      <td>2</td>\n",
       "      <td>564</td>\n",
       "      <td>282</td>\n",
       "      <td>0.577844</td>\n",
       "      <td>0.996466</td>\n",
       "      <td>0.769854</td>\n",
       "      <td>0.969459</td>\n",
       "      <td>0.616371</td>\n",
       "      <td>0.731061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.85</td>\n",
       "      <td>368</td>\n",
       "      <td>1</td>\n",
       "      <td>565</td>\n",
       "      <td>300</td>\n",
       "      <td>0.550898</td>\n",
       "      <td>0.998233</td>\n",
       "      <td>0.756078</td>\n",
       "      <td>0.969459</td>\n",
       "      <td>0.597656</td>\n",
       "      <td>0.709740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.90</td>\n",
       "      <td>329</td>\n",
       "      <td>1</td>\n",
       "      <td>565</td>\n",
       "      <td>339</td>\n",
       "      <td>0.492515</td>\n",
       "      <td>0.998233</td>\n",
       "      <td>0.724473</td>\n",
       "      <td>0.969459</td>\n",
       "      <td>0.552477</td>\n",
       "      <td>0.659319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.95</td>\n",
       "      <td>146</td>\n",
       "      <td>1</td>\n",
       "      <td>565</td>\n",
       "      <td>522</td>\n",
       "      <td>0.218563</td>\n",
       "      <td>0.998233</td>\n",
       "      <td>0.576175</td>\n",
       "      <td>0.969459</td>\n",
       "      <td>0.333484</td>\n",
       "      <td>0.358282</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    threshold   TP  FP   TN   FN       sen       spe       Acc       AUC  \\\n",
       "0        0.05  609  44  522   59  0.911677  0.922261  0.916532  0.969459   \n",
       "1        0.10  577  25  541   91  0.863772  0.955830  0.905997  0.969459   \n",
       "2        0.15  558  17  549  110  0.835329  0.969965  0.897083  0.969459   \n",
       "3        0.20  545  15  551  123  0.815868  0.973498  0.888169  0.969459   \n",
       "4        0.25  534  13  553  134  0.799401  0.977032  0.880875  0.969459   \n",
       "5        0.30  521  12  554  147  0.779940  0.978799  0.871151  0.969459   \n",
       "6        0.35  508   9  557  160  0.760479  0.984099  0.863047  0.969459   \n",
       "7        0.40  498   9  557  170  0.745509  0.984099  0.854943  0.969459   \n",
       "8        0.45  483   7  559  185  0.723054  0.987633  0.844408  0.969459   \n",
       "9        0.50  465   6  560  203  0.696108  0.989399  0.830632  0.969459   \n",
       "10       0.55  450   6  560  218  0.673653  0.989399  0.818476  0.969459   \n",
       "11       0.60  439   5  561  229  0.657186  0.991166  0.810373  0.969459   \n",
       "12       0.65  433   4  562  235  0.648204  0.992933  0.806321  0.969459   \n",
       "13       0.70  419   3  563  249  0.627246  0.994700  0.795786  0.969459   \n",
       "14       0.75  401   2  564  267  0.600299  0.996466  0.782010  0.969459   \n",
       "15       0.80  386   2  564  282  0.577844  0.996466  0.769854  0.969459   \n",
       "16       0.85  368   1  565  300  0.550898  0.998233  0.756078  0.969459   \n",
       "17       0.90  329   1  565  339  0.492515  0.998233  0.724473  0.969459   \n",
       "18       0.95  146   1  565  522  0.218563  0.998233  0.576175  0.969459   \n",
       "\n",
       "         MCC        F1  \n",
       "0   0.832503  0.922029  \n",
       "1   0.817040  0.908661  \n",
       "2   0.804404  0.897828  \n",
       "3   0.790044  0.887622  \n",
       "4   0.778804  0.879012  \n",
       "5   0.763249  0.867610  \n",
       "6   0.751972  0.857384  \n",
       "7   0.738950  0.847660  \n",
       "8   0.723752  0.834197  \n",
       "9   0.703130  0.816506  \n",
       "10  0.684497  0.800712  \n",
       "11  0.673135  0.789568  \n",
       "12  0.668001  0.783710  \n",
       "13  0.653302  0.768807  \n",
       "14  0.634085  0.748833  \n",
       "15  0.616371  0.731061  \n",
       "16  0.597656  0.709740  \n",
       "17  0.552477  0.659319  \n",
       "18  0.333484  0.358282  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label, pred = myEval(modelCD, device, valided_loader)\n",
    "evaluation_df(pred, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pos_arr = np.array(pred)[np.array(label)==1]\n",
    "neg_arr = np.array(pred)[np.array(label)==0]\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.hist(pos_arr, bins=100, label='pos', color='cornflowerblue')\n",
    "plt.legend()\n",
    "plt.subplot(1,2,2)\n",
    "plt.hist(neg_arr, bins=100, label='neg', color='coral')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test on 6k data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>threshold</th>\n",
       "      <th>TP</th>\n",
       "      <th>FP</th>\n",
       "      <th>TN</th>\n",
       "      <th>FN</th>\n",
       "      <th>sen</th>\n",
       "      <th>spe</th>\n",
       "      <th>Acc</th>\n",
       "      <th>AUC</th>\n",
       "      <th>MCC</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.05</td>\n",
       "      <td>1186</td>\n",
       "      <td>2277</td>\n",
       "      <td>2614</td>\n",
       "      <td>149</td>\n",
       "      <td>0.888390</td>\n",
       "      <td>0.534451</td>\n",
       "      <td>0.610344</td>\n",
       "      <td>0.856633</td>\n",
       "      <td>0.349301</td>\n",
       "      <td>0.494373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.10</td>\n",
       "      <td>1138</td>\n",
       "      <td>1787</td>\n",
       "      <td>3104</td>\n",
       "      <td>197</td>\n",
       "      <td>0.852434</td>\n",
       "      <td>0.634635</td>\n",
       "      <td>0.681336</td>\n",
       "      <td>0.856633</td>\n",
       "      <td>0.400539</td>\n",
       "      <td>0.534272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.15</td>\n",
       "      <td>1089</td>\n",
       "      <td>1313</td>\n",
       "      <td>3578</td>\n",
       "      <td>246</td>\n",
       "      <td>0.815730</td>\n",
       "      <td>0.731548</td>\n",
       "      <td>0.749598</td>\n",
       "      <td>0.856633</td>\n",
       "      <td>0.461426</td>\n",
       "      <td>0.582820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.20</td>\n",
       "      <td>1042</td>\n",
       "      <td>1007</td>\n",
       "      <td>3884</td>\n",
       "      <td>293</td>\n",
       "      <td>0.780524</td>\n",
       "      <td>0.794112</td>\n",
       "      <td>0.791198</td>\n",
       "      <td>0.856633</td>\n",
       "      <td>0.501914</td>\n",
       "      <td>0.615839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.25</td>\n",
       "      <td>986</td>\n",
       "      <td>813</td>\n",
       "      <td>4078</td>\n",
       "      <td>349</td>\n",
       "      <td>0.738577</td>\n",
       "      <td>0.833776</td>\n",
       "      <td>0.813363</td>\n",
       "      <td>0.856633</td>\n",
       "      <td>0.518243</td>\n",
       "      <td>0.629228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.30</td>\n",
       "      <td>962</td>\n",
       "      <td>658</td>\n",
       "      <td>4233</td>\n",
       "      <td>373</td>\n",
       "      <td>0.720599</td>\n",
       "      <td>0.865467</td>\n",
       "      <td>0.834404</td>\n",
       "      <td>0.856633</td>\n",
       "      <td>0.548235</td>\n",
       "      <td>0.651100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.35</td>\n",
       "      <td>940</td>\n",
       "      <td>575</td>\n",
       "      <td>4316</td>\n",
       "      <td>395</td>\n",
       "      <td>0.704120</td>\n",
       "      <td>0.882437</td>\n",
       "      <td>0.844202</td>\n",
       "      <td>0.856633</td>\n",
       "      <td>0.561031</td>\n",
       "      <td>0.659649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.40</td>\n",
       "      <td>911</td>\n",
       "      <td>524</td>\n",
       "      <td>4367</td>\n",
       "      <td>424</td>\n",
       "      <td>0.682397</td>\n",
       "      <td>0.892864</td>\n",
       "      <td>0.847735</td>\n",
       "      <td>0.856633</td>\n",
       "      <td>0.560616</td>\n",
       "      <td>0.657762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.45</td>\n",
       "      <td>869</td>\n",
       "      <td>457</td>\n",
       "      <td>4434</td>\n",
       "      <td>466</td>\n",
       "      <td>0.650936</td>\n",
       "      <td>0.906563</td>\n",
       "      <td>0.851751</td>\n",
       "      <td>0.856633</td>\n",
       "      <td>0.558874</td>\n",
       "      <td>0.653138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.50</td>\n",
       "      <td>831</td>\n",
       "      <td>400</td>\n",
       "      <td>4491</td>\n",
       "      <td>504</td>\n",
       "      <td>0.622472</td>\n",
       "      <td>0.918217</td>\n",
       "      <td>0.854802</td>\n",
       "      <td>0.856633</td>\n",
       "      <td>0.557173</td>\n",
       "      <td>0.647701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.55</td>\n",
       "      <td>762</td>\n",
       "      <td>326</td>\n",
       "      <td>4565</td>\n",
       "      <td>573</td>\n",
       "      <td>0.570787</td>\n",
       "      <td>0.933347</td>\n",
       "      <td>0.855606</td>\n",
       "      <td>0.856633</td>\n",
       "      <td>0.544846</td>\n",
       "      <td>0.628972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.60</td>\n",
       "      <td>727</td>\n",
       "      <td>294</td>\n",
       "      <td>4597</td>\n",
       "      <td>608</td>\n",
       "      <td>0.544569</td>\n",
       "      <td>0.939890</td>\n",
       "      <td>0.855124</td>\n",
       "      <td>0.856633</td>\n",
       "      <td>0.536999</td>\n",
       "      <td>0.617148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.65</td>\n",
       "      <td>698</td>\n",
       "      <td>272</td>\n",
       "      <td>4619</td>\n",
       "      <td>637</td>\n",
       "      <td>0.522846</td>\n",
       "      <td>0.944388</td>\n",
       "      <td>0.853999</td>\n",
       "      <td>0.856633</td>\n",
       "      <td>0.528762</td>\n",
       "      <td>0.605640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.70</td>\n",
       "      <td>680</td>\n",
       "      <td>240</td>\n",
       "      <td>4651</td>\n",
       "      <td>655</td>\n",
       "      <td>0.509363</td>\n",
       "      <td>0.950930</td>\n",
       "      <td>0.856248</td>\n",
       "      <td>0.856633</td>\n",
       "      <td>0.532349</td>\n",
       "      <td>0.603104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.75</td>\n",
       "      <td>647</td>\n",
       "      <td>202</td>\n",
       "      <td>4689</td>\n",
       "      <td>688</td>\n",
       "      <td>0.484644</td>\n",
       "      <td>0.958700</td>\n",
       "      <td>0.857051</td>\n",
       "      <td>0.856633</td>\n",
       "      <td>0.530220</td>\n",
       "      <td>0.592491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.80</td>\n",
       "      <td>598</td>\n",
       "      <td>153</td>\n",
       "      <td>4738</td>\n",
       "      <td>737</td>\n",
       "      <td>0.447940</td>\n",
       "      <td>0.968718</td>\n",
       "      <td>0.857051</td>\n",
       "      <td>0.856633</td>\n",
       "      <td>0.525058</td>\n",
       "      <td>0.573346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.85</td>\n",
       "      <td>544</td>\n",
       "      <td>107</td>\n",
       "      <td>4784</td>\n",
       "      <td>791</td>\n",
       "      <td>0.407491</td>\n",
       "      <td>0.978123</td>\n",
       "      <td>0.855766</td>\n",
       "      <td>0.856633</td>\n",
       "      <td>0.517224</td>\n",
       "      <td>0.547835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.90</td>\n",
       "      <td>484</td>\n",
       "      <td>66</td>\n",
       "      <td>4825</td>\n",
       "      <td>851</td>\n",
       "      <td>0.362547</td>\n",
       "      <td>0.986506</td>\n",
       "      <td>0.852714</td>\n",
       "      <td>0.856633</td>\n",
       "      <td>0.504810</td>\n",
       "      <td>0.513528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.95</td>\n",
       "      <td>377</td>\n",
       "      <td>44</td>\n",
       "      <td>4847</td>\n",
       "      <td>958</td>\n",
       "      <td>0.282397</td>\n",
       "      <td>0.991004</td>\n",
       "      <td>0.839062</td>\n",
       "      <td>0.856633</td>\n",
       "      <td>0.446886</td>\n",
       "      <td>0.429385</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    threshold    TP    FP    TN   FN       sen       spe       Acc       AUC  \\\n",
       "0        0.05  1186  2277  2614  149  0.888390  0.534451  0.610344  0.856633   \n",
       "1        0.10  1138  1787  3104  197  0.852434  0.634635  0.681336  0.856633   \n",
       "2        0.15  1089  1313  3578  246  0.815730  0.731548  0.749598  0.856633   \n",
       "3        0.20  1042  1007  3884  293  0.780524  0.794112  0.791198  0.856633   \n",
       "4        0.25   986   813  4078  349  0.738577  0.833776  0.813363  0.856633   \n",
       "5        0.30   962   658  4233  373  0.720599  0.865467  0.834404  0.856633   \n",
       "6        0.35   940   575  4316  395  0.704120  0.882437  0.844202  0.856633   \n",
       "7        0.40   911   524  4367  424  0.682397  0.892864  0.847735  0.856633   \n",
       "8        0.45   869   457  4434  466  0.650936  0.906563  0.851751  0.856633   \n",
       "9        0.50   831   400  4491  504  0.622472  0.918217  0.854802  0.856633   \n",
       "10       0.55   762   326  4565  573  0.570787  0.933347  0.855606  0.856633   \n",
       "11       0.60   727   294  4597  608  0.544569  0.939890  0.855124  0.856633   \n",
       "12       0.65   698   272  4619  637  0.522846  0.944388  0.853999  0.856633   \n",
       "13       0.70   680   240  4651  655  0.509363  0.950930  0.856248  0.856633   \n",
       "14       0.75   647   202  4689  688  0.484644  0.958700  0.857051  0.856633   \n",
       "15       0.80   598   153  4738  737  0.447940  0.968718  0.857051  0.856633   \n",
       "16       0.85   544   107  4784  791  0.407491  0.978123  0.855766  0.856633   \n",
       "17       0.90   484    66  4825  851  0.362547  0.986506  0.852714  0.856633   \n",
       "18       0.95   377    44  4847  958  0.282397  0.991004  0.839062  0.856633   \n",
       "\n",
       "         MCC        F1  \n",
       "0   0.349301  0.494373  \n",
       "1   0.400539  0.534272  \n",
       "2   0.461426  0.582820  \n",
       "3   0.501914  0.615839  \n",
       "4   0.518243  0.629228  \n",
       "5   0.548235  0.651100  \n",
       "6   0.561031  0.659649  \n",
       "7   0.560616  0.657762  \n",
       "8   0.558874  0.653138  \n",
       "9   0.557173  0.647701  \n",
       "10  0.544846  0.628972  \n",
       "11  0.536999  0.617148  \n",
       "12  0.528762  0.605640  \n",
       "13  0.532349  0.603104  \n",
       "14  0.530220  0.592491  \n",
       "15  0.525058  0.573346  \n",
       "16  0.517224  0.547835  \n",
       "17  0.504810  0.513528  \n",
       "18  0.446886  0.429385  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_feature = pd.read_csv(\"./myData/Test6k/labeled.csv\").values\n",
    "label_target  = pd.read_csv(\"./myData/Test6k/true_label.csv\").values.reshape(-1)\n",
    "\n",
    "test_loader = myTrainPrep.labeled_batch_creator(label_feature, label_target, 300, -1)\n",
    "label, pred = myEval(modelCD, device, test_loader)\n",
    "evaluation_df(pred, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pos_arr = np.array(pred)[np.array(label)==1]\n",
    "neg_arr = np.array(pred)[np.array(label)==0]\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.hist(pos_arr, bins=100, label='pos', color='cornflowerblue')\n",
    "plt.ylim(0,550)\n",
    "plt.legend()\n",
    "plt.subplot(1,2,2)\n",
    "plt.hist(neg_arr, bins=100, label='neg', color='coral')\n",
    "plt.ylim(0,550)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_1.6.0",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
