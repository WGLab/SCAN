{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the device is cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "device    = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('the device is %s' % device)\n",
    "\n",
    "from myLibs.myEval import myEval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[date]-[name]\n"
     ]
    }
   ],
   "source": [
    "shuffle_seed      = 37\n",
    "pos_size          = 250\n",
    "neg_size          = 270\n",
    "train_size        = 1000\n",
    "BATCHSIZE        = 500\n",
    "\n",
    "num_epochs = 10\n",
    "prefix     = \"[date]-[name]\"\n",
    "print(prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "label_feature = pd.read_csv(\"myData/Train/labeled.csv\").values\n",
    "label_target  = pd.read_csv(\"myData/Train/true_label.csv\").values.reshape(-1)\n",
    "\n",
    "pos_feature = label_feature[label_target == 1]\n",
    "pos_label   = label_target[label_target == 1]\n",
    "np.random.seed(shuffle_seed)\n",
    "np.random.shuffle(pos_feature)\n",
    "np.random.seed(shuffle_seed)\n",
    "np.random.shuffle(pos_label)\n",
    "\n",
    "neg_feature = label_feature[label_target == 0]\n",
    "neg_label   = label_target[label_target == 0]\n",
    "np.random.seed(shuffle_seed)\n",
    "np.random.shuffle(neg_feature)\n",
    "np.random.seed(shuffle_seed)\n",
    "np.random.shuffle(neg_label)\n",
    "\n",
    "train_x = np.array(list(pos_feature[0:pos_size]) +list(neg_feature[0:neg_size]))\n",
    "train_y = np.array(list(pos_label[0:pos_size]) + list(neg_label[0:neg_size]))\n",
    "valid_x = np.array(list(pos_feature[pos_size:]) + list(neg_feature[neg_size:]))\n",
    "valid_y = np.array(list(pos_label[pos_size:]) + list(neg_label[neg_size:]))\n",
    "\n",
    "trainDataset = TensorDataset(torch.Tensor(label_feature[:, np.newaxis, :]), torch.Tensor(label_target).long())\n",
    "labeledLoader = DataLoader(dataset=trainDataset, batch_size = BATCHSIZE, shuffle=True)\n",
    "\n",
    "validDataset = TensorDataset(torch.Tensor(valid_x[:, np.newaxis, :]), torch.Tensor(valid_y).long())\n",
    "validLoader  = DataLoader(dataset=validDataset, batch_size = BATCHSIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabel_data  = pd.read_csv(\"myData/Train/unlabel.csv\").values\n",
    "unlabeledDataset = TensorDataset(torch.Tensor(unlabel_data[:, np.newaxis, :]))\n",
    "unlabeledLoader  = DataLoader(dataset=unlabeledDataset, batch_size=BATCHSIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from myLibs.myDis import NetD\n",
    "from myLibs.myGen import NetG\n",
    "modelCD     = NetD().to(device)\n",
    "modelG      = NetG().to(device)\n",
    "criterionC  = nn.CrossEntropyLoss()\n",
    "optimizerCD = optim.Adam(modelCD.parameters(), lr=0.00095, betas=(0.5, 0.999), weight_decay = .01)\n",
    "optimizerG  = optim.Adam(modelG.parameters(),  lr=0.00095, betas=(0.5, 0.999), weight_decay = .01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN1d_5layer(\n",
      "  (cnn1): Sequential(\n",
      "    (0): Conv1d(1, 10, kernel_size=(3,), stride=(1,))\n",
      "    (1): BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): Tanh()\n",
      "    (3): Dropout(p=0.6, inplace=False)\n",
      "  )\n",
      "  (cnn2): Sequential(\n",
      "    (0): Conv1d(10, 40, kernel_size=(3,), stride=(1,))\n",
      "    (1): BatchNorm1d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): Tanh()\n",
      "    (3): Dropout(p=0.6, inplace=False)\n",
      "  )\n",
      "  (cnn3): Sequential(\n",
      "    (0): Conv1d(40, 200, kernel_size=(3,), stride=(1,))\n",
      "    (1): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (linear1): Sequential(\n",
      "    (0): Linear(in_features=13000, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "NetG(\n",
      "  (Linear1): Sequential(\n",
      "    (0): Linear(in_features=30, out_features=128, bias=True)\n",
      "    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): LeakyReLU(negative_slope=0.01)\n",
      "    (3): Dropout(p=0.6, inplace=False)\n",
      "  )\n",
      "  (linear2): Sequential(\n",
      "    (0): Linear(in_features=128, out_features=256, bias=True)\n",
      "    (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): LeakyReLU(negative_slope=0.01)\n",
      "    (3): Dropout(p=0.6, inplace=False)\n",
      "  )\n",
      "  (linear3): Sequential(\n",
      "    (0): Linear(in_features=256, out_features=512, bias=True)\n",
      "    (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): LeakyReLU(negative_slope=0.01)\n",
      "    (3): Dropout(p=0.6, inplace=False)\n",
      "  )\n",
      "  (linear4): Sequential(\n",
      "    (0): Linear(in_features=512, out_features=71, bias=True)\n",
      "    (1): BatchNorm1d(71, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): Tanh()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(modelCD)\n",
    "print(modelG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [01:14<00:00,  7.50s/it]\n"
     ]
    }
   ],
   "source": [
    "from tensorboardX import SummaryWriter\n",
    "writer = SummaryWriter(comment=\"%s\" % prefix)\n",
    "\n",
    "maxacc = 0\n",
    "\n",
    "\n",
    "global_step = 1\n",
    "iter_labeled = iter(labeledLoader)\n",
    "\n",
    "\n",
    "for epoch in tqdm.tqdm(range(num_epochs)):\n",
    "    for step, x_unlabeled in enumerate(unlabeledLoader):\n",
    "\n",
    "        #################################################################################  Classifier/Discriminator \n",
    "        modelCD.train()\n",
    "        modelG.eval()\n",
    "        \n",
    "        optimizerCD.zero_grad()\n",
    "        \n",
    "        ## label\n",
    "        try:\n",
    "            x_labeled, y_labeled = next(iter_labeled)\n",
    "            x_labeled, y_labeled = x_labeled.to(device), y_labeled.to(device)\n",
    "        except StopIteration:\n",
    "            iter_labeled = iter(labeledLoader)\n",
    "            x_labeled, y_labeled = next(iter_labeled)\n",
    "            x_labeled, y_labeled = x_labeled.to(device), y_labeled.to(device)\n",
    "        \n",
    "        \n",
    "        _, outClassLabeled  = modelCD(x_labeled)\n",
    "        lossLabeled      = criterionC(outClassLabeled, y_labeled)\n",
    "        \n",
    "        \n",
    "        ## unlabel\n",
    "        x_unlabeled = x_unlabeled[0].to(device)\n",
    "        _, outClassUnlabeled  = modelCD(x_unlabeled)\n",
    "        \n",
    "        logz_unlabeled = torch.logsumexp(outClassUnlabeled, dim=1)\n",
    "        lossUnlabeled  = -0.5 * torch.mean(logz_unlabeled) + 0.5 * torch.mean(F.softplus(logz_unlabeled))\n",
    "        \n",
    "        ## Fake\n",
    "        fakeNoise1       = torch.randn(x_unlabeled.size(0), 30, device=device)\n",
    "        x_Fake1          = ( modelG(fakeNoise1) + 1.0 ) / 2\n",
    "        _, outClassFake1 = modelCD(x_Fake1)\n",
    "\n",
    "        logz_fake1 = torch.logsumexp(outClassFake1, dim=1)\n",
    "        lossFake  = 0.5 * torch.mean(F.softplus(logz_fake1))\n",
    "        \n",
    "\n",
    "        ## loss\n",
    "        totalLoss = lossLabeled + lossUnlabeled + lossFake\n",
    "        \n",
    "        ## optimization\n",
    "        writer.add_scalar(\"training_loss/supervised\", lossLabeled, global_step)\n",
    "        writer.add_scalar(\"training_loss/unsupervised\", lossUnlabeled+lossFake, global_step)\n",
    "        writer.add_scalar(\"training_loss/D_Loss\", totalLoss, global_step)\n",
    "\n",
    "        totalLoss.backward()\n",
    "        optimizerCD.step()\n",
    "        \n",
    "                \n",
    "        #################################################################################  Generator\n",
    "        modelCD.eval()\n",
    "        modelG.train()\n",
    "        optimizerG.zero_grad()\n",
    "        \n",
    "        fakeNoise2 = torch.randn(x_unlabeled.size(0), 30, device=device)\n",
    "        x_Fake2    = ( modelG(fakeNoise2) + 1.0 ) / 2\n",
    "        \n",
    "        ## loss\n",
    "        y_pred_unlabeled, _ = modelCD(x_unlabeled)\n",
    "        y_pred_fake, _      = modelCD(x_Fake2)\n",
    "        mom_real = torch.mean(y_pred_unlabeled, dim=0)\n",
    "        mom_fake = torch.mean(y_pred_fake, dim=0)\n",
    "        diff = mom_fake * 100 - mom_real * 100\n",
    "        lossG = torch.mean(diff * diff)\n",
    "        \n",
    "        \n",
    "        ## optimization\n",
    "        writer.add_scalar(\"training_loss/G_loss\", lossG, global_step)\n",
    "        lossG.backward()        \n",
    "        optimizerG.step()\n",
    "\n",
    "        global_step += 1\n",
    "  \n",
    "    \n",
    "    \n",
    "    train_loss, train_accuracy = myEval(modelCD, criterionC, device, labeledLoader, False)\n",
    "    writer.add_scalar(\"accuracy/train\", train_accuracy, epoch)\n",
    "\n",
    "    \n",
    "    if epoch >= 1000 and train_accuracy > maxacc:\n",
    "        torch.save(modelCD.state_dict(), './model_save/%s_%s.pt' % (prefix, epoch))\n",
    "        maxacc = train_accuracy\n",
    "\n",
    "\n",
    "    \n",
    "writer.flush()\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(modelCD.state_dict(), './model_save/%s.pt' % prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_1.6.0",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}